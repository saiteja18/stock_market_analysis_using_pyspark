# Financial Market Analysis Leveraging PySpark And Spark MLlib

## Data Collection: 
The "Data scraping.ipynb" notebook serves as a comprehensive script for scraping financial data from the web. It begins by acquiring a list of symbols traded on NASDAQ, cleans the data by removing test issues, and then proceeds to download historical price data. It meticulously validates the downloaded data, ensuring completeness and accuracy. The notebook also meticulously organizes the collected data, categorizing symbols into separate directories based on whether they represent ETFs or stocks. Additionally, it exports a refined list of valid symbols along with their metadata to a CSV file for future reference. This notebook not only automates the data scraping process but also ensures data integrity.

## EDA:
The "EDA_stock_data.ipynb" notebook conducts exploratory data analysis (EDA) on stock data using PySpark. It begins by setting up the environment, loading the data from CSV files stored in Google Drive, and structuring it into a Data Frame. The notebook then delves into extensive data exploration, including statistical summaries, checks for missing values, and visualizations such as histograms and heatmaps to understand data distributions and correlations. Additionally, it performs advanced analyses like calculating maximum stock prices, yearly high and low values, and average prices per month, providing insights into the behavior of individual stocks over time.

Furthermore, the notebook employs data manipulation techniques to extract temporal features like year, month, and week from the date column. It conducts groupings and joins to derive yearly and weekly high and low values, facilitating deeper insights into the temporal dynamics of stock prices. Finally, it exports the processed data into a CSV file, enabling seamless visualization and analysis in Power BI. This integration allows for leveraging Power BI's robust visualization capabilities to gain deeper insights and communicate findings effectively with stakeholders. Overall, the notebook serves as a comprehensive guide to exploring, analyzing, and deriving insights from large-scale stock datasets using PySpark, making it a valuable resource for financial analysts and researchers alike.

## Volatility of Stocks: 
The "Volatility_of_Stocks.ipynb" script calculates the volatility of stock prices from a collection of CSV files containing historical stock data. It first defines functions to parse CSV lines, calculate returns, and determine volatility. Then, it iterates over CSV files in a specified folder, extracts adjusted close prices, computes returns, and subsequently calculates volatility for each stock. The script then sorts the stocks based on their volatility and outputs the top 10 and bottom 10 volatile stocks. Additionally, it writes the volatility data to a CSV file for further analysis.

Afterward, the script utilizes PySpark to load the volatility data from the CSV file into a Data Frame. It showcases the Data Frame’s schema and the first few rows, computes the range of volatility (minimum and maximum), and generates a histogram to visualize the distribution of volatility values. Overall, the script provides a comprehensive analysis of stock volatility using both conventional Python and PySpark functionalities, making it a valuable tool for understanding and exploring stock market dynamics.
## Stock Prediction Using LSTM:
The Prediction Using LSTM.ipynb script offers a robust pipeline for analyzing stock data and implementing a Long Short-Term Memory (LSTM) model for stock price prediction, blending the capabilities of PySpark and Keras. It initiates by loading stock data from a CSV file using PySpark, followed by a visualization step where a scatter plot showcases the relationship between time and the adjusted close value of the stocks. Subsequently, the script performs feature scaling utilizing PySpark's MinMaxScaler to standardize the input data, ensuring that each feature contributes equally to the model's training process.

Next, the script transitions to a Pandas Data Frame for easier manipulation and compatibility with Keras, a Python deep-learning library. It splits the data into training and testing sets using TimeSeriesSplit from scikit-learn, facilitating sequential partitioning suitable for time series data. The LSTM model is then constructed and trained using Keras, leveraging its sequential architecture to incorporate LSTM layers for capturing temporal dependencies in the stock data. The model is optimized using Adam optimizer and evaluated based on its performance metrics, such as root mean squared error (RMSE), providing insights into the accuracy of the predicted stock prices.

In the final stages, the LSTM model makes predictions on the test data, and these predictions are visualized alongside the true values to assess the model's efficacy in capturing the underlying patterns in the stock prices. By integrating PySpark's data processing capabilities with Keras' deep learning functionalities, the script offers a comprehensive framework for both analyzing historical stock data and building predictive models for future price movements, making it a valuable asset for financial analysts and researchers aiming to harness the power of machine learning in stock market forecasting.

## Stock Prediction Using Classification:
The Stock Price Prediction.ipynb code outlines a comprehensive data preprocessing, feature engineering, and modeling pipeline for analyzing stock data using PySpark. It begins by loading the data into a Spark Data Frame, followed by preprocessing steps such as converting the Ticker column to a unique identifier code and extracting date components. Feature engineering techniques are then applied to calculate the balance, categorize the balance as positive or negative, and take the absolute value of the balance.

After preparing the data, a subset is selected for train-test split, and three classification models—Random Forest, Logistic Regression, and Decision Tree—are implemented using a pipeline approach. Each model is trained, tuned using hyperparameters and cross-validation, and evaluated based on the F1 score metric, considering the imbalanced nature of the dataset. The best performing model, Random Forest, is further validated on the entire dataset, demonstrating its superiority in predicting stock price movements over Logistic Regression and Decision Tree models.
In conclusion, the implementation showcases a systematic approach to model selection, hyperparameter tuning, and evaluation, emphasizing the significance of Random Forest in accurately predicting stock price movements. The F1 score serves as a robust evaluation metric, ensuring the models' effectiveness in handling class imbalance and providing insights into their overall performance on unseen data.

